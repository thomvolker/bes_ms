---
title: |
  | PROPOSAL
  | 
  | 
  |
  |
  |
  | **Bayesian Evidence Synthesis**
  |
  |
  |
  |
  |
  |
  |
author: |
  | Thom Volker (5868777)
  |
  |
  | Supervisor: Irene Klugkist
  |
  |
  |
  | *Methodology and Statistics for the Behavioural, Biomedical and Social Sciences*
  |
  | *Utrecht University*
  |
  |
  |
  | `r format(Sys.time(), '%B %d, %Y')`
  |
  |
  | `r paste0("Word count: ", wordcountaddin::word_count())`
  |
  |
  |
  | *Candidate journals: Statistical Science; Sociological Methods & Research*
output: 
  bookdown::pdf_document2:
    number_sections: true
    df_print: kable
    highlight: pygments
    toc: false
mainfont: Calibri
sansfont: Calibri
linestretch: 2
fontsize: 10pt
params: 
  !r Sys.setlocale("LC_TIME", "C")
date: 
indent: true
tables: true
header-includes:
  - \usepackage{caption}
bibliography: "../../thesis_literature.bib"
csl: "../../apa-6th-edition.csl"
link-citations: yes
linkcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


<!--
\noindent
\textcolor{darkgreen}{Ik vraag me af of dit niet een te uitgebreide inleiding voor een proposal is, omdat de limiet maar 750 woorden is. Ik kan dit ook bewaren voor de daadwerkelijke inleiding, en het proposal heel kort inleiding met iets als: onderzoekers maken nou eenmaal bewust en onbewust verschillende keuzes met eenzelfde onderzoeksvraag. Wanneer dit het geval is, is het lastig om het bewijs te aggregeren, omdat meta-analyse hier niet goed toe in staat is. Een alternatieve methode is BES, die gebruikt kan worden om ... }
-->

# Introduction

<!--\textcolor{darkgreen}{Word count is inclusief comments, zonder comments is de word count 479.}-->

In recent years, the importance of replications has received considerable attention [e.g., @open_science_collab_2015; @baker_reproducibility_2016; @brandt_et_al_replication_2014]. However, emphasis has been placed primarily on exact, direct or close replication studies. These studies employ an identical methodology and research design as the initial study, and are thus merely concerned with the statistical reliability of the results. If these results depend on methodological flaws, inferences from all studies will lead to suboptimal or invalid conclusions [@munafo_robust_2018]. To overcome these limitations, the use of conceptual replications has been advocated [e.g., @munafo_robust_2018; @lawlor_triangulation_2017]. Specifically, conceptual replications scrutinize the extent to which the initial conclusions hold under different conditions, using varying instruments or operationalizations.

However, established methods such as (Bayesian) meta-analysis and Bayesian updating are not applicable when studies differ conceptually. This is due to the fact that these methods require that the parameter estimates (i) share a common scale, and (ii) result from analyses with identical functional forms [@lipsey_wilson_2001; @sutton_bayesian_meta2001; @schonbrodt_sequential_2017]. Consequently, @kuiper_combining_2013 proposed Bayesian Evidence Synthesis (BES), which is built upon the foundation of the Bayes Factor [BF; @kass_raftery_bayes_factors_1995]. This method allows researchers to pool evidence for a specific hypothesis over multiple studies, even if the studies have seemingly incompatible designs. 

<!--
First, one proceeds by constructing study-specific hypotheses that reflect a more general hypothesis (i.e., scientific theory). Note that the study-specific hypotheses may differ over the studies, as long as all reflect the same overarching general theory. Subsequently, the support for the study-specific hypotheses can be expressed in terms of a BF. BFs render the support for the hypothesis at hand, relative to an alternative hypothesis, for which conveniently an unconstrained or complement hypothesis can be used. Loosely speaking, the BF expresses how much more likely the hypothesis at hand is, compared to the chosen alternative. Ultimately, all study-specific BFs can be multiplied, to express the support for the overall hypothesis in one measure of evidence [@kuiper_combining_2013]. -->

The work by @kuiper_combining_2013 and @behrens_2019 has led to the implementation of the method in applied research [e.g., @zondervan_robust_2020; @zondervan_parental_2019], and provided the building blocks for the current project. Under general conditions, BES adequately evaluates inequality constrained (i.e., informative) hypotheses, but shows problems when equality constrained hypotheses are evaluated [a thorough overview about the distinction is presented by @hoijtink_informative_2012]. Equality constrained hypotheses become particularly problematic when, at least, one of the studies lacks statistical power. Additionally, BFs are highly dependent on the complexity of a given hypothesis [i.e., the number of parameters that are incorporated into the hypothesis; @klugkist_inequality_2005; @mulder_equality_2010]. As studies differ conceptually, the complexity of the hypotheses that address the same overarching theory in different studies may also differ. Currently, it is not known to what extent these issue affect the performance of BES, let alone how these might be overcome.

# The current project

The foremost goal of the current project is to reveal under which circumstances BES performs inadequately. Additionally, we hope to propose adjustments to the method that improve its performance. We will do so by employing simulations in which data will be generated and analysed according to multivariable linear, logit and probit models, to reflect varying study-designs that are often encountered in sociological research [e.g., @kuiper_combining_2013; @buskens_raub_embedded_2002]. Each of the datasets generated by one of these statistical models represents a single "study" in which a set of candidate hypotheses will be evaluated. We consider situations in which all "studies" assess sets of hypotheses with equal complexities, and situations in which the complexities of the hypotheses within the set differ between the "studies". Additionally, we distinguish between situations in which the set of candidate hypotheses contains the true hypothesis (i.e., the data-generating model) within all "studies", and simulations that only consider incorrect hypotheses. The BFs for all candidate hypotheses will be calculated by means of the R-package `bain` [@bain], after which the study-specific BFs can be multiplied manually to obtain the combined BF. 

We consider sample sizes that increase incrementally from $n = 25$ to $n = 500$ in steps of $25$, although we distinguish between situations in which all studies employ identical sample sizes, and situations in which one study remains fixed at $n = 25$. Additionally, all simulations will be conducted for small, medium and large effect sizes [@cohen_1988]. By varying the complexity of the hypotheses and the sample sizes, we hope to pinpoint desirable and undesirable effects of between-study differences on the performance of BES. The actual performance will be deduced from the true hypothesis rate (THR; the proportion of all simulations in which the "true" hypothesis has the largest combined BF). Desirably, the THR will converge towards 1 as the sample size and effect size increase. Situations in which the THR deviates from this desirable effect will determine where additional simulation settings are required, and will guide the focus of potential improvements of the method. Ethical consent has been provided by the FERB.


# References

\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\noindent

<div id="refs"></div>
