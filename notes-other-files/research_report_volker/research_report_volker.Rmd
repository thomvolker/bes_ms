---
title: |
  Bayesian Evidence Synthesis
type: "RESEARCH REPORT - Word count: `r wordcountaddin::word_count() - 900 # for formulas`"
author:
  - name: Thom Benjamin Volker
    affil: 1, 2 
    email: t.b.volker@uu.nl
affiliation:
  - num: 1
    address: |
      Supervised by Prof. Dr. Irene Klugkist
  - num: 2
    address: |
      Utrecht University
bibliography: "../../thesis.bib"
#appendix: appendix.tex
# abstract: |
#   Paper Bayesian Evidence Synthesis - this is a placeholder abstract this is a placeholder abstract this is a placeholder abstract this is a placeholder abstract this is a placeholder abstract this is a placeholder abstract this is a placeholder abstract this is a placeholder abstract this is a placeholder abstract this is a placeholder abstract this is a placeholder abstract this is a placeholder abstract this is a placeholder abstract this is a placeholder abstract this is a placeholder abstract this is a placeholder abstract this is a placeholder abstract this is a placeholder abstract
keywords: |
  Bayes factors, Evidence Aggregation, hypothesis evaluation
header-includes: |
  \usepackage[hidelinks]{hyperref}
  \usepackage[utf8]{inputenc}
  \def\tightlist{}
  \usepackage{booktabs}
  \usepackage{adjustbox}
output: rticles::tf_article
#csl: "../../apa-6th-edition.csl" # "/Users/thomvolker/Documents/styles/nature.csl"  #

---

```{r setup, include = FALSE}
library(xtable)
options(xtable.floating = TRUE)
options(xtable.timestamp = "")
options(xtable.comment = FALSE)
```

# Introduction

In recent years, a meta-analytic way of thinking has been advocated in the scientific community. This approach is grounded in the belief that a single study is merely contributing to a larger body of evidence [e.g., @asendorpf_recommendations_2016; @cumming_new_2014; @goodman_reproducibility_2016; @schmidt_replication_2009]. 
In this context, the importance of replication has been legitimately supported [e.g., @baker_reproducibility_2016; @brandt_et_al_replication_2014; @munafo_manifesto_2017]. 
However, most of the attention has been focused on studies that are highly similar, using an identical methodology and research design [e.g., @camerer2016evaluating; @camerer2018evaluating; @klein_etal_replicability_2014; @nosek_replicability_review_2021; @open_science_collab_2015]. 
These studies, commonly referred to as exact, direct or close replications, are merely concerned with the statistical reliability of the results. 
If the results of the studies depend on methodological flaws, inferences from all studies will lead to suboptimal or invalid conclusions [@lawlor_triangulation_2017; @munafo_robust_2018]. 

Conceptual replications, which primarily assess the validity of a study, protect against placing too much confidence in such flawed findings. 
Specifically, conceptual replications are a way of investigating whether the conclusions hold under alternative conditions, using varying measurement instruments or other operationalizations [@nosek_scientific_2012]. 
Different methodologies may have different strengths and weaknesses, that may affect the final conclusion that can be drawn from the data. 
Combining evidence from multiple approaches mitigates the effect of these strengths and weaknesses, which enhances the validity and the robustness of the final conclusion [@lawlor_triangulation_2017; @lipton2003inference; @mathison1988triangulate; @munafo_robust_2018; @nosek_scientific_2012]. 

In the conventional framework of direct replications, combining evidence over studies is straightforward, because established methods as (Bayesian) meta-analysis or Bayesian sequential updating can be applied to aggregate the results [@lipsey_wilson_2001; @schonbrodt_sequential_2017; @sutton_bayesian_meta2001].
These methods pool the parameter estimates or effect sizes obtained in the individual studies [@cooper_handbook_2009]. 
However, if the studies differ considerably with regard to operationalizations of key variables or statistical models used, the parameter estimates or effect sizes will not be comparable. 
Accordingly, an aggregate of these estimates cannot be meaningfully interpreted, rendering the use of conventional methods unfeasible. 

To overcome these difficulties, @kuiper_combining_2013 proposed a new method called *Bayesian Evidence Synthesis* (*BES*). 
At its core, *BES* quantifies the support for an overall scientific theory or an overarching hypothesis, by aggregating Bayes factors obtained at the level of individual studies. 
In every single study, a hypothesis can be formulated that reflects an overall theory, but that incorporates data characteristics and methodologies unique to that study.
The support for each of these study-specific hypotheses can be expressed using a Bayes factor (BF), rendering the relative support for the hypothesis of interest over an alternative hypothesis [@kass_raftery_bayes_factors_1995].
If the study-specific hypotheses represent the same underlying (i.e., latent) effect, the Bayes factors in favor of these hypotheses can be meaningfully combined.

Although *BES* has been applied successfully to substantive research questions [e.g., @kevenaar_bes_2021; @zondervan_parental_2019; @zondervan_robust_2020], hardly any empirical investigation into the method's performance has been conducted. 
As a consequence, researchers may be hesitant to implement the method in practice, because the required conditions to achieve good performance are relatively unknown.
In this paper, it will be assessed how *BES* behaves empirically when combining evidence from studies that employ different statistical models, by focusing on whether *BES* indeed favors true hypotheses over alternatives. 
Given the fact that studies with low statistical power (i.e., $60\%)$ are omnipresent in social science research [e.g., @ingre_power_2018; @button2013power], it will be assessed to what extent *BES* is vulnerable to power issues.
Hence, this paper aims to assess how well *BES* performs when aggregating results of different analysis techniques, under varying sample sizes and effect sizes. 


<!-- One of the properties of the Bayes factor is that it is highly dependent on the complexity of the candidate hypotheses (i.e., the fraction of the parameter space that is concerned by the hypotheses). -->
<!-- However, the hypotheses reflecting the same overall theory may differ in their complexity due to different operationalizations, which affects the study-specific Bayes factors.  -->
<!-- For example, one of the studies may measure a construct using a categorized variable, while another study uses a continuous scale, or one study measures a construct using a factor score, while another includes the separated variables into their analysis.  -->
<!-- Additionally, study-specific Bayes factors are highly dependent on the power of the corresponding analyses. -->
<!-- The effect of these characteristics on the aggregated amount of evidence for the overall theory is not known.  -->
<!-- The goal of this paper is to assess to what extent sample size, effect size and complexity of the hypothesis affects the ultimate amount of evidence for the hypothesis under evaluation, using a simulation study. -->

# Methods

## Informative hypotheses

At it's core, *BES* aims to quantify the evidence for a scientific theory, by aggregating the relative amount of evidence for the hypothesis of interest over multiple studies.
A scientific theory is often best described by an informative hypothesis, which, contrary to the classical null hypothesis testing approach, allows to make scientific expectations explicit [e.g., see @hoijtink_informative_2012; @vandeschoot_informative_2011].^[Other advantages of evaluating informative hypotheses are numerous  [e.g., see @beland2012informative; @hoijtink_klugkist_boelen_2008; @klaassen_informative_2020; @vandeschoot_informative_sem_2011; @vandeschoot_informative_introduction_2011].]
As an example, consider a research group investigating whether researchers who experience more publication pressure are more often involved in scientific misconduct [e.g., @gopalakrishna_riet_vink_stoop_wicherts_bouter_2021].
When putting this theory to the test, the research group may measure the amount of publication pressure researchers experience and ask how often these researchers were involved in scientific misconduct, and quantify both measures on a continuous scale.
Formalizing the theory as a testable informative hypothesis would yield 
$$
\mathcal{H}_{1}: \beta_{p} > 0,
$$
where the hypothesis of interest $\mathcal{H}_1$ states that the effect of publication pressure on scientific misconduct $\beta_{p}$ is positive. 
Another researcher might have data in which the outcome, involvement in scientific misconduct, only consists of two categories (i.e., "yes" or "no"), but that contains two measures for publication pressure that are both expected to be positively related to scientific misconduct: the expected necessity of top-tier publications for successful grant applications and the expected necessity of top-tier publications for getting tenure. 
An expectation might then be that both sources of publication pressure are positively related to scientific misconduct, and that the effect of both is actually similar, because they are equally important to researchers.
Capturing this expectation in an informative hypothesis would yield
$$
\mathcal{H}_{2}: \{\beta_{p_1} = \beta_{p_2}\} > 0,
$$
where $\beta_{p_1}$ and $\beta_{p_2}$ indicate the effect of publication pressure through grant applications and getting tenure on scientific misconduct, respectively.

## Bayes factors

The Bayes Factor [@kass_raftery_bayes_factors_1995] can be used to quantify the support for informative hypotheses [@beland2012informative; @hoijtink_informative_2012; @hoijtink2019tutorial]. Loosely speaking, a Bayes factor expresses the relative support from the data for an hypothesis of interest versus some alternative hypothesis. Returning to the example, the Bayes factor for hypothesis $\mathcal{H}_1$ versus an alternative hypothesis $\mathcal{H}_a$ is defined as the ratio of the marginal likelihoods of the data under these hypotheses
$$
BF_{1,a} = \frac{P(D|\mathcal{H}_1)}{P(D|\mathcal{H}_a)}.
$$
A Bayes factor that equals $7$ indicates that after seeing the data, $\mathcal{H}_1$ is $7$ times more likely than $\mathcal{H}_a$. When the alternative hypothesis is unconstrained (i.e., $\mathcal{H}_a = \mathcal{H}_u$), the expression of the Bayes factor boils down to
$$
BF_{1,u} = \frac{f_1}{c_1},
$$
where $f_1$ indicates the fit of the data to $\mathcal{H}_1$ and $c_1$ indicates the complexity of the hypothesis $\mathcal{H}_1$ [e.g., @beland2012informative; @klugkist_inequality_2005]. A similar approach can be used when the hypothesis of interest contains equality constraints (like hypothesis $\mathcal{H}_2$ in the example), but requires some additional steps that will not be discussed here [see e.g., @mulder_equality_2010; @klugkist_inequality_2005]. When the alternative hypothesis of interest is another informative hypothesis $\mathcal{H}_{3}$ (e.g., $\beta_p < 0$), the same formulation of the Bayes factor can be applied to both hypotheses, which is generally easier than calculating the marginal likelihoods under both hypotheses [@klugkist_inequality_2005], providing
$$
BF_{1,3} = \frac{BF_{1,u}}{BF_{3,u}}.
$$
Combining the obtained Bayes factors with prior model probabilities (i.e., the probability that the hypotheses under consideration are true *a priori*), allows to calculate posterior model probabilities. The posterior model probabilities for hypothesis $\mathcal{H}_i$, with $i \in \{1, 2, \dots, m\}$, are given by
$$
PMP(\mathcal{H}_{i}) = \frac{\pi_i BF_{i,u}}{\sum^m_{i'=1} \pi_{i'} BF_{i',u}},
$$
where $\pi_i$ indicates the prior model probability of hypothesis $H_i$. The posterior model probabilities indicate the relative support for the hypotheses under consideration.

## Bayesian evidence synthesis

Regardless of differences in study design and analysis plan, a Bayes factor can be calculated to express the support for the theory in each of the studies under consideration, as long as the hypotheses under consideration are conceptually similar. To aggregate the evidence for a theory over multiple studies, *BES* uses the possibility to specify the prior model probabilities. After every study that is conducted, the support for the hypothesis of interest can be expressed as a posterior model probability. These posterior model probabilities after study $j$ can be used as prior model probabilities in study $j + 1$ [@kuiper_combining_2013]. Independent of the order of updating, repeating this process for $J$ studies yields
$$
PMP(\mathcal{H}_i)^J = \frac{\pi^0_{i} \prod^J_{j=1} BF^j_{i,u}}{\sum^m_{i'=1} \pi^0_{i'} \prod^J_{j=1} BF^j_{i',u}},
$$
where $\pi^0_i$ indicate the prior model probabilities for hypothesis $\mathcal{H}_i$ before a study has been conducted. Because one could argue that before observing any data all hypotheses are equally likely, $\pi^0_i$ is usually the same for all hypotheses (i.e., $\pi^0_i = \frac{1}{m}$). If the initial prior model probabilities are the same for all hypotheses, the product of Bayes factors equals the aggregated posterior model odds (the numerator of $PMP(\mathcal{H}_i)^J$), which contains the same information as the aggregated posterior model probabilities. These measures then indicate the relative support for the theory (i.e., overall hypothesis) in all studies simultaneously, irrespective of the specifics of each study.

## Simulation

A simulation study is conducted in `R` [@R\; Version 4.1.0] to assess how the outcome of *BES* is affected by sample size and effect size.
Because *BES* can be applied regardless of between-study heterogeneity, data to represent three different studies is generated using three models: ordinary least squares (OLS), logistic and probit regression.
In all studies, the *true* hypothesis $\mathcal{H}_1: \beta_1 < \beta_2 < \beta_3 < \beta_4$ is evaluated against an unconstrained alternative $\mathcal{H}_u: \beta_1, \beta_2, \beta_3, \beta_4$ using the `R`-package `BFpack` [@BFpack\; Version 0.3.2], and *BES* is used to pool the evidence over studies. 
Because the initial prior model probabilities are specified equally (i.e., $\pi^0_1 = \pi^0_u = \frac{1}{2}$), and the product of Bayes factors is log-transformed for interpretability, a product that is greater than $0$ yields more support for $\mathcal{H}_1$ than for $\mathcal{H}_u$.

Each study consists of a continuous or binary outcome $Y$ and a predictor matrix $X$, with dimensions $n \times 4$.
The sample sizes vary with $n \in \{25, 50, 100, 200, 400, 800\}$, while the effect sizes take on the values $R^2 \in \{0.02, 0.09, 0.25\}$, in accordance with small, medium and large effects as defined by @cohen_1988. For binary outcomes, McKelvey and Zavoina's $R^2_{M\&Z}$ is used [@mckelvey_zavoina_1975], which closely resembles the conventional $R^2$ empirically [@hagle_mitchell_goodness_1992; @demaris_explained_2002].
Two sets of simulations will be considered: in the first, the sample sizes and effect sizes are consistent across studies, while in the second, the sample size of a single, randomly selected, study is kept fixed at $n = 25$. All simulation conditions are evaluated over $1000$ iterations.
All predictor variables are normally distributed with a mean of $0$, a variance of $1$ and a common covariance of $0.3$, and their relation with the outcome $Y$ is specified according to the weights-matrix $B = [1,2,3,4]^T$, such that $\beta_4 = 4\beta_1, \beta_3 = 3\beta_1, \beta_2 = 2\beta_1$.
Accordingly the exact coefficients can be defined as
$$
\beta = B \Bigg(\sqrt{\frac{\text{Var}(\hat{Y})}{G^T ((B B^T) \odot \Sigma) G }}\Bigg),
$$
where $G$ is a $4 \times 1$ column-vector of ones and $\text{Var}(\hat{Y}) = \text{Var}(X\beta)$ is defined as a function of the effect size.^[Note that $\text{Var}(\hat{Y}) = R^2$ when $Y$ is continuous with a variance of $\sigma^2_Y = 1$, while $\text{Var}(\hat{Y}) = \frac{R^2\frac{\pi^2}{3}}{(1 - R^2)}$ and $\text{Var}(\hat{Y}) = \frac{R^2}{1 - R^2}$ for logistic and probit regression, respectively.] 
The population-level regression coefficients are displayed in Table \ref{tab:coefs}.
When the outcome variable is continuous, $Y$ is drawn from a normal distribution
$$
Y \sim \mathcal{N}(X\beta, 1 - R^2),
$$
with a mean vector of $X\beta$ and residual variance $\sigma^2 = 1-R^2$. When $Y$ is binary, $Y$ is drawn from a Bernoulli distribution
$$
Y \sim \mathcal{B}(p),
$$
and $p$ equals 
$$
p_{\text{logit}} = \frac{1}{e^{-X\beta}}, 
~~~~~~~~~
p_{\text{probit}} = \Phi(X\beta),
$$
for logistic and probit regression, respectively, and with $\Phi$ indicating the cumulative normal distribution. 


```{r, echo = FALSE, message = FALSE, warning = FALSE, results = 'asis'}
library(tidyverse)
coefs <- function(r2, ratio, rho, model = c("normal", "logit", "probit")) {
  
  if (model == "normal") {
    var_y <- r2
  }
  else if (model == "logit") {
    var_y <- (r2 * pi^2 / 3) / (1 - r2)
  }
  else if (model == "probit") {
    var_y <- r2 / (1 - r2)
  }
  
  sqrt(var_y / sum(ratio %*% t(ratio) * rho)) * ratio
}

ratio <- c(1,2,3,4)
cormat <- diag(4)
cormat[cormat != 1] <- 0.3
R2 <- c("R^2: 0.02" = 0.02, 
        "R^2: 0.09" = 0.09, 
        "R^2: 0.25" = 0.25)

models <- c("Normal" = "normal", "Logistic" = "logit", "Probit" = "probit")

table_coefs <- function(R2, models, ratio, rho) {
  coef <- coefs(R2, ratio, rho, models) %>% sprintf(fmt = "%.2f") %>% t %>% as_tibble
  coef
}


betas <- expand_grid(R2, models) %>%
  mutate(coefs = map2(R2, models, ~table_coefs(.x, .y, ratio, cormat))) %>%
  unnest() %>%
  mutate(V0 = rep(" ", nrow(.))) %>%
  pivot_wider(names_from = models, 
              values_from = c("V0", "V1", "V2", "V3", "V4")) %>%
  mutate(R2 = as.character(R2)) %>%
  select(R2, map(paste0(c("normal", "logit", "probit")), ~paste0("V", 0:4, "_", .x)) %>% unlist())

beta_names <- c(" ", rep(c(" ", "$\\beta_1$", "$\\beta_2$", "$\\beta_3$", "$\\beta_4$"), 3)) %>%
  t %>%
  as_tibble

names(beta_names) <- colnames(betas)

betas <- bind_rows(beta_names, betas) %>%
  mutate("Empty" = rep(" ", nrow(.)), .after = R2)

addtorow <- list()
addtorow$pos <- list(0)
addtorow$command <- c("$R^2$ & & & \\multicolumn{4}{c}{OLS} & & \\multicolumn{4}{c}{Logistic} & & \\multicolumn{4}{c}{Probit} \\\\\n")
  
xtable(betas, 
       auto = TRUE,
       caption = c("Population-level regression coefficients for ordinary least squares (OLS), logistic and probit regression, given effect sizes of $R^2 \\in \\{0.02, 0.09, 0.25\\}$."),
       label = c("tab:coefs")) %>%
  print(sanitize.text.function = function(x) {x},
        include.rownames = FALSE,
        include.colnames = FALSE,
        add.to.row = addtorow,
        booktabs = TRUE,
        hline.after = c(-1, 0, 1, nrow(.)),
        scalebox = 0.9,
        caption.placement = "top",
        table.placement = "ht")
```
\vspace{-8mm}

# Results


```{r sim1, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE}
load("../simulations/01_sample_size_equal.RData")

hypnames <- c("Hi", "Complement", "Unconstrained")

output1 <- output %>%
  mutate(bf_u  = map(fit,  ~c(.x[,7], unconstrained = 1)),
         bf_h1 = map_dbl(bf_u, ~.x[1]),
         pmp_u = map(bf_u, ~.x / sum(.x)),
         r2 = paste0("R^2: ", r2))

BF1 <- output1 %>%
  group_by(nsim, n, r2) %>%
  summarize(bf_h1 = prod(bf_h1), .groups = "drop") %>%
  ggplot(aes(x = as.factor(n), y = log(bf_h1), fill = as.factor(n))) +
  geom_boxplot() +
  geom_hline(yintercept = 0, col = "grey", size = 1) +
  facet_wrap(~r2, labeller = label_parsed) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  labs(y = expression(log~BF["H"["1,u"]]),
       x = "Sample size",
       title = "Combined Bayes factor over studies") +
  theme(legend.position = "none")

THR1 <- output1 %>%
  mutate(bf_u = map(bf_u, ~as_tibble(matrix(.x,
                                            ncol = 3,
                                            dimnames = list(NULL,
                                                            hypnames))))) %>%
  unnest(bf_u) %>%
  group_by(nsim, n, r2) %>%
  summarise(across(c(Hi, Complement, Unconstrained), prod), .groups = "drop") %>%
  mutate(Hi = ifelse(Hi > Unconstrained, 1, 0),
         Unconstrained = ifelse(Unconstrained > Hi, 1, 0)) %>%
  group_by(n, r2) %>%
  summarize(thr = mean(Hi), .groups = "drop") %>%
  ggplot(aes(x = n, y = thr, col = r2)) +
  geom_line() +
  scale_color_brewer(palette = "Set1", labels = scales::parse_format()) +
  theme_minimal() +
  labs(y = "True hypothesis rate",
       x = "Sample size",
       col = bquote(R^2),
       title = "True hypothesis rate of hypothesis of interest versus unconstrained.")

rm(list = c("output", "output1"))
```

```{r sim2, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE}
load("../simulations/02_sample_size_underpowered.RData")

output2 <- output %>%
  mutate(bf_u  = map(fit,  ~c(.x[,7], unconstrained = 1)),
         bf_h1 = map_dbl(bf_u, ~.x[1]),
         pmp_u = map(bf_u, ~.x / sum(.x)),
         r2 = paste0("R^2: ", r2))

BF2 <- output2 %>%
  group_by(nsim, n_initial, r2) %>%
  summarize(bf_h1 = prod(bf_h1), .groups = "drop") %>%
  ggplot(aes(x = as.factor(n_initial), 
             y = log(bf_h1), 
             fill = as.factor(n_initial))) +
  geom_boxplot() +
  geom_hline(yintercept = 0, col = "grey", size = 1) +
  facet_wrap(~r2, labeller = label_parsed) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  labs(y = expression(log~BF["H"["1,u"]]),
       x = "Sample size",
       title = "Combined Bayes Factor") +
  theme(legend.position = "none")

THR2 <- output2 %>%
  mutate(bf_u = map(bf_u, ~as_tibble(matrix(.x,
                                            ncol = 3,
                                            dimnames = list(NULL,
                                                            hypnames))))) %>%
  unnest(bf_u) %>%
  group_by(nsim, n_initial, r2) %>%
  summarise(across(c(Hi, Complement, Unconstrained), prod), .groups = "drop") %>%
  mutate(Hi = ifelse(Hi > Unconstrained, 1, 0),
         Unconstrained = ifelse(Unconstrained > Hi, 1, 0)) %>%
  group_by(n_initial, r2) %>%
  summarize(thr = mean(Hi), .groups = "drop") %>%
  ggplot(aes(x = n_initial, y = thr, col = r2)) +
  geom_line() +
  scale_color_brewer(palette = "Set1", labels = scales::parse_format()) +
  theme_minimal() +
  labs(y = "True hypothesis rate",
       x = "Sample size",
       col = bquote(R^2),
       title = "True hypothesis rate of hypothesis of interest versus unconstrained")

rm(list = c("output", "output2"))
```

```{r fig1, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "\\label{fig1}Logarithm of the product of Bayes factors for the true hypothesis of interest versus an unconstrained hypothesis over three studies (generated with ordinary least squares (OLS), logistic and probit regression), for multiple sample sizes and effect sizes. In Simulation 2 the sample size of one randomly selected study is kept fixed at $n = 25$.", fig.height=5, out.width='100%'}
library(patchwork)

patchwork::wrap_plots(BF1 + labs(title = "Simulation 1"), 
                      BF2 + labs(title = "Simulation 2"), ncol = 1)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, fig.cap = "\\label{fig2}True hypothesis rate of $BES$ over three studies (generated with ordinary least squares (OLS), logistic and probit regression), for multiple sample sizes and effect sizes. In Simulation 2 the sample size of randomly selected study is kept fixed at $n = 25$.", fig.height=3, out.width = '100%'}
patchwork::wrap_plots(THR1 + labs(title = "Simulation 1") + theme(legend.position = "none") + ylim(0,1),
                      THR2 + labs(title = "Simulation 2") + ylim(0,1))
```

The simulations show that the aggregated Bayes factors generally tend to increase with the sample size and effect size. In Simulation 1 in Figure \ref{fig1}, the log-transformed product of Bayes factor over the three studies $\text{log}(BF_{1,u})$ is centered above zero only when the sample size equals $n = 800$ when the effect is small (i.e., $R^2 = 0.02$). Hence, with a small effect size, only the largest sample size considered renders more support for $\mathcal{H}_1$ than for $\mathcal{H}_u$ in the majority of the simulations. When $R^2 = 0.09$ or $R^2 = 0.25$, $\text{log}(BF_{1,u})$ is centered around zero when $n = 100$ and $n = 25$, respectively, and shows convincing support for the true hypothesis $\mathcal{H}_1$ in nearly all simulations for large sample sizes.

In Simulation 2 in Figure \ref{fig1}, where one of the three studies is randomly selected to have a sample size of only $n = 25$ observations, $\text{log}(BF_{1,u})$ is somewhat lower overall as compared to Simulation 1 (ignoring those simulations in which the sample size of all studies equals $n = 25$). This is to be expected, because with $n = 25$ the unconstrained hypothesis generally obtains more support than hypothesis $\mathcal{H}_1$, for all effect sizes. Apart from this, a similar trend can be observed, in the sense that the aggregated Bayes factors tend to increase with sample size and effect size. However, the point from which the true hypothesis $\mathcal{H}_1$ receives most support in the majority of the simulations (i.e., the point from which $\text{log}(BF_{1,u})$ is centered above zero) is generally reached later than in Simulation 1, and $\mathcal{H}_1$ is convincingly supported in most simulations only for large sample sizes (i.e., $n \geq 400$) and a large effect (i.e., $R^2 = 0.25$).

A similar picture is shown in Figure \ref{fig2}, which displays the true hypothesis rates for the hypothesis of interest $\mathcal{H}_1$ using the product of Bayes factors for both simulation set-ups. The true hypothesis rate increases with the sample size and effect size. In Simulation 1, when $n = 25$ the true hypothesis receives most support in only a minority of the simulations for all effect sizes, while for $n = 100$ only the studies with medium or large effects render most support for $\mathcal{H}_1$ in the majority of the simulations (i.e., about 50% and 80%, respectively). For $n = 800$ all effect sizes yield most support for the true hypothesis in the majority of simulations. When the set of studies contains a single study with a small sample size (i.e., $n = 25$; in Simulation 2), the increase in the true hypothesis rate is less steep and $\mathcal{H}_1$ is supported less often. Apart from this, the same general patterns can be observed as in Simulation 1.

# Discussion and conclusion

Overall, *BES* showed to have satisfactory properties, in the sense that it tends to select the true hypothesis when the power of the studies is sufficiently large. In that sense, it proves to be a welcome addition to conventional methods of research synthesis, like Bayesian updating of parameter estimates or (Bayesian) meta-analysis. In contrast to these methods, *BES* can be applied when the study designs differ (e.g., through the use of different statistical models, as presented here). However, rather than pooled parameter estimates, *BES* provides the relative support for the hypotheses under consideration over the entire set of studies simultaneously. That is, *BES* quantifies to what extent the theory of interest is supported in each of these studies. 

Consequently, unlike meta-analysis and Bayesian sequential updating, which assess to what extent all studies together provide evidence for a hypothesis, *BES* cannot solve power issues. In fact, if the set of studies contains a single, severely underpowered, study, this might already affect the performance of *BES*. Our second simulation indeed suggests that including a single underpowered study, which is the case for studies with a sample size of $n = 25$ and a regression model containing $4$ predictors, lowers the performance of *BES* substantially. This finding warrants further research into the severity of the effect of statistical power on the performance of *BES*. Moreover, it provides a cautionary note for researchers who want to apply *BES*: a synthesis of studies that lack statistical power might yield invalid conclusions.

The current study focused exclusively on the effects of the sample size and effect size on the performance of *BES*. However, other factors may likewise affect the outcome of *BES*. When aggregating results of different studies, the complexities of the study-specific hypotheses may differ due different operationalizations of key variables. As Bayes factors reflect a trade-off between the fit of the data to the hypothesis and the complexity of the hypothesis, differences in complexity may affect the outcome of *BES*. Additionally, in our simulations, we compared our hypothesis of interest with an unconstrained alternative hypothesis. However, evaluating against the classical null hypothesis, another informative hypothesis or the complement hypothesis can be reasonable choices, too. To what extent such factors will influence the performance of *BES* is not yet understood. 

<!-- To assess the influence of certain data characteristics on the ultimate amount of evidence for the hypothesis of interest, a simulation study will be performed. -->
<!-- The main goal of the simulations is to assess to what extent the complexity of the hypothesis of interest affects the aggregated *BES* measure within a regression framework. -->
<!-- The complexity of the hypothesis of interest will be adjusted in two ways: (i) by creating a combined variable by averaging three distinct independent variables, and (ii) by splitting a continuous variable into a categorical variable with $3$ categories. -->
<!-- One of the appealing characteristics of *BES* is that it can be applied regardless of the method of analysis in the available studies [@kuiper_combining_2013]. -->
<!-- The performance will be evaluated by combining the Bayes factors concerning the hypothesis of interest over three studies, in which the generated data is analyzed using an ordinary least squares (OLS) regression model, a logistic regression model and a probit regression model. -->
<!-- Hence, in all analyses, the outcome variable $Y$, which can be continuous or binary, is regressed on a set of predictors $\bf{X}$, with dimensions $n \times k$.  -->
<!-- The number of observations takes on the values $n \in \{25, 50, 100, 200, 400, 800\}$ and is consistent over the simulations.  -->
<!-- In general, the three studies from which the results are aggregated have the same sample size. -->
<!-- The number of predictors $k$ varies between the simulation conditions, but is also the same within the studies. -->
<!-- Hence, the three studies that are aggregated differ only in terms of their outcome variable $Y$. -->

<!-- # Results -->

<!-- ## The Effect of Power -->

<!-- Simulation 1 - varying effect size and sample size only (not too informative, but shows what can be expected, and is a nice way to show what is actually done without having to explain everything at once). -->

<!-- ## Combining Three Variables into One Average -->

<!-- Simulations 3 and 4 - model three predictors as three separate predictors versus modeling three predictors as a single mean score. -->

<!-- ## Splitting One Variables into Three Categories -->

<!-- Simulation 6 - model a single continuous variable as is (i.e., a single continuous variable) versus modeling a categorized version of this variable with three distinct categories. -->

<!-- # Discussion -->

<!-- Discussion -->




